{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3a6c1c",
   "metadata": {},
   "source": [
    "# Mid Term 1 - KNN & Logistic Regression\n",
    "*Authors*:  Aliotta Lorenzo, Prataiolo Loris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0568c336-c64c-4229-ac7e-30786a3d52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b19bbdd-35a5-40bc-bf48-608fa35c28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = np.load(\"synthetic_linear_gaussian/linsep_d2_n200.npz\")     # (d = 2 ;  n = 200)\n",
    "ds2 = np.load(\"synthetic_linear_gaussian/linsep_d10_n1000.npz\")   # (d = 10;  n = 1000)\n",
    "ds3 = np.load(\"synthetic_linear_gaussian/linsep_d500_n50000.npz\") # (d = 500; n = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd62d936-b54d-4eee-a141-dbcfb82abc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_train', 'y_train', 'X_test', 'y_test', 'flip_mask', 'noise_rate', 'seed', 'feature_dim', 'n_samples', 'test_size']\n"
     ]
    }
   ],
   "source": [
    "print(ds1.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a93d3cf-7c4f-401e-8895-e148ed7dc1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print(ds1[\"feature_dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02776674-6e88-4d61-994a-95dc8fbb5690",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c774e5c-aefd-438f-a828-066498b83d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidDistance(P1,P2):\n",
    "    return np.linalg.norm(P1-P2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97ab50ad-eda0-4a41-8a7f-12828c6f7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Ytrue):\n",
    "    return (np.count_nonzero(Ypred != Ytrue)) / len(Ytrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c83bd694-fb0a-4ce0-97fc-3fa221fb6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allDistances(X1, X2):\n",
    "    '''\n",
    "    Compute pairwise Euclidean distnaces between two X sets.\n",
    "    output: D :=  A matrix (X1_rows x X2_rows) where D[i, j] equals the Euclidean distance \n",
    "                  between the i-th sample in X1 and the j-th sample in X2.\n",
    "    '''\n",
    "    D = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "\n",
    "    for i in range(X1.shape[0]):\n",
    "        for j in range (X2.shape[0]):\n",
    "            D[i,j] = euclidDistance(X1[i,:], X2[j,:])\n",
    "            \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b5a8704-721d-49c7-874c-cea8d094ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassify(Xtr, Ytr, k, Xte):\n",
    "\n",
    "    n_train = Xtr.shape[0]  # number of the training inputs\n",
    "    n_test  = Xte.shape[0]  # number of the test inputs\n",
    "\n",
    "    # sanity checks\n",
    "    if any(np.abs(Ytr) != 1):\n",
    "        raise Exception(\"The values of Ytr should be +1 or -1.\")\n",
    "\n",
    "    if k > n_train:\n",
    "        print(\"k is greater than the number of points, setting k=n_train\")\n",
    "        k = n_train\n",
    "\n",
    "    Ypred = np.zeros(n_test)\n",
    "\n",
    "    # Compute all the distances from TEST input and TRAINING input\n",
    "    dist = allDistances(Xte,Xtr)\n",
    "    \n",
    "    # For each test point, the predicted class will be \n",
    "    # the sign of the average label of the k nearest points\n",
    "    for idx in range(n_test):\n",
    "\n",
    "        # Take all distances for the current test point and sort them \n",
    "        idx_dist = dist[idx, :]     \n",
    "        idx_sort = np.argsort(idx_dist)\n",
    "        \n",
    "        # Get the indices of the k lowest of distances\n",
    "        k_dist_index = idx_sort[:k]\n",
    "        \n",
    "        # Compute the mean of the output values of the corresponding training points and taking the sign of it.\n",
    "        mean = np.mean(Ytr[k_dist_index])\n",
    "\n",
    "        Ypred[idx] = np.sign(mean) # returns -1 if x < 0, 0 if x==0, 1 if x > 0\n",
    "    return Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25025be6-8b56-4279-afa6-9e0228904fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCVkNN(Xtr, Ytr, num_folds, k_list, rng: int | Generator | None = None):\n",
    "    \"\"\"Run K-Fold CV for the kNN model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "     - Xtr : np.array\n",
    "         the full training set data\n",
    "     - Ytr : np.array\n",
    "         the full training set labels\n",
    "     - num_folds : int\n",
    "         the number of folds\n",
    "     - k_list : List[int]\n",
    "         the values of k (for k-NN) to try\n",
    "     - rng : Optional[int | Generator | None]\n",
    "         optional random state\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "     - best_k : int\n",
    "         The value of k (in k_list) which obtains the best average validation error\n",
    "     - best_k_idx : int\n",
    "         The index of the best_k element in k_list\n",
    "     - tr_err_mean : np.array\n",
    "         A 1D array of the same length as k_list, with the average training error for each tested k.\n",
    "     - tr_err_std : np.array\n",
    "         A 1D array of the same length as k_list, with the standard deviation \n",
    "         of the training error for each tested k.\n",
    "     - val_err_mean : np.array\n",
    "         A 1D array of the same length as k_list, with the average validation error for each tested k.\n",
    "     - val_err_std : np.array\n",
    "         A 1D array of the same length as k_list, with the standard deviation\n",
    "         of the validation error for each tested k.\n",
    "    \"\"\"\n",
    "    rng = _check_random_generator(rng)\n",
    "    # Ensures that k_list is a numpy array\n",
    "    k_list = np.array(k_list)\n",
    "    num_k = len(k_list)\n",
    "\n",
    "    n_tot = Xtr.shape[0]\n",
    "\n",
    "    # We want to compute 1 error for each `k` and each fold\n",
    "    tr_errors = np.zeros((num_k, num_folds)) # train\n",
    "    val_errors = np.zeros((num_k, num_folds)) # validation\n",
    "\n",
    "    # `split_idx`: a list of arrays, each containing the validation indices for 1 fold\n",
    "    rand_idx = rng.choice(n_tot, size=n_tot, replace=False)\n",
    "    split_idx = np.array_split(rand_idx, num_folds) \n",
    "    \n",
    "    for fold_idx in range(num_folds):\n",
    "        # Set the indices in boolean mask for all validation samples to `True`\n",
    "        val_mask = np.zeros(n_tot, dtype=bool)\n",
    "        val_mask[split_idx[fold_idx]] = True\n",
    "        \n",
    "        # Split training set in training part and validation part\n",
    "        x_train = Xtr[val_mask==False]\n",
    "        y_train = Ytr[val_mask==False]\n",
    "        x_val = Xtr[val_mask==True]\n",
    "        y_val = Ytr[val_mask==True]\n",
    "        \n",
    "        for k_idx, current_k in enumerate(k_list):\n",
    "            # TODO: Compute the training error of the kNN classifier for the given value of k\n",
    "            Tpred = kNNClassify(x_train, y_train, current_k, x_train)\n",
    "            tr_errors[k_idx, fold_idx] = calcError(Tpred, y_train)\n",
    "            \n",
    "            # TODO: Compute the validation error of the kNN classifier for the given value of k\n",
    "            Vpred = kNNClassify(x_train, y_train, current_k, x_val)\n",
    "            val_errors[k_idx, fold_idx] = calcError(Vpred, y_val)\n",
    "            \n",
    "    # Calculate error statistics along the repetitions:\n",
    "    # 1) mean training error, training error standard deviation\n",
    "    tr_err_mean = np.mean(tr_errors, axis=1)\n",
    "    tr_err_std = np.std(tr_errors, axis=1)\n",
    "    # 2) mean validation error, validation error standard deviation\n",
    "    val_err_mean = np.mean(val_errors, axis=1)\n",
    "    val_err_std = np.std(val_errors, axis=1)\n",
    "    # 3) best k (k which minimize mean validation error) and index of best k in k_list\n",
    "    best_k = k_list[np.argmin(val_err_mean)]\n",
    "    best_k_idx = np.where(k_list == best_k)\n",
    "    \n",
    "    return best_k, best_k_idx, tr_err_mean, tr_err_std, val_err_mean, val_err_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef6bac-139f-4359-9fa1-2450a8b325b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_knn_process(dataset):\n",
    "\n",
    "    # Get all data\n",
    "    Xtr = dataset['X_train']\n",
    "    Xte = dataset['X_test']\n",
    "    Ytr = dataset['y_train']\n",
    "    Yte = dataset['y_test']\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85471c-b6e8-4bca-8dff-5bc8f8eb088f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
